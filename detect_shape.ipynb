{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "We'll use tensorflow to predict the type of shape in each image: oval, diamond or bean.\n",
    "\n",
    "We should already have `.npy` files in `greyscale-data`.  We'll first load data into various structures for later.  This cell mainly splits the data into training, validation and test folds.  To make the splits I'm hashing filenames and then sorting those hashes alphabetically -- this mixes up the images, but makes the mixing deterministic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_directory = 'greyscaled-data'\n",
    "\n",
    "# Load all the data into an array.\n",
    "# Each element is a tuple: (filename, numpy data).\n",
    "# The filename structure is \"<number>-<color>-<texture>-<shape>-<rotation>.png\"\n",
    "all_data = [\n",
    "  (f, np.load(os.path.join(input_directory, f))) for f in os.listdir(input_directory)\n",
    "]\n",
    "\n",
    "# Hash the filename and sort the hashes alphabetically.\n",
    "all_data_with_hashes = [\n",
    "  (filename, hashlib.md5(filename).hexdigest(), data) for filename, data in all_data\n",
    "]\n",
    "all_data_sorted = sorted(all_data_with_hashes, key=lambda element: element[1])\n",
    "\n",
    "# Save 20% of the data for testing (the final, one-shot evaluation of performance).\n",
    "split_index = int(0.2 * len(all_data_sorted))\n",
    "test_data = all_data_sorted[0:split_index]\n",
    "remaining_data = all_data_sorted[split_index:]\n",
    "\n",
    "# Now save 20% of the remaining data for validation.\n",
    "split_index = int(0.2 * len(remaining_data))\n",
    "validation_data = remaining_data[0:split_index]\n",
    "training_data = remaining_data[split_index:]\n",
    "\n",
    "# For convenience, get all the pixel data into separate arrays.\n",
    "training_pixel_data = [pixel_data for _, _, pixel_data in training_data]\n",
    "validation_pixel_data = np.array([pixel_data for _, _, pixel_data in validation_data])\n",
    "test_pixel_data = np.array([pixel_data for _, _, pixel_data in test_data])\n",
    "\n",
    "# Each filename, in its text, has an embedded type of shape.\n",
    "# As in, \"2-red-empty-oval-45.npy\"\n",
    "# We need to convert those classes (the output ground truth) into label arrays.\n",
    "all_labels = {\n",
    "  'oval': [1., 0., 0.],\n",
    "  'diamond': [0., 1., 0.],\n",
    "  'bean': [0., 0., 1.],\n",
    "}\n",
    "training_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in training_data\n",
    "]\n",
    "validation_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in validation_data\n",
    "]\n",
    "test_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "setup tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "learning_rate = 0.00005\n",
    "regularization_factor = 1e-4\n",
    "card_width, card_height = 150, 150\n",
    "first_hidden_layer_size, second_hidden_layer_size = 1024, 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Setup the training steps.\n",
    "  tf_training_data = tf.placeholder(tf.float32, shape=[None, card_width*card_height])\n",
    "  tf_training_labels = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "  \n",
    "  # Create hidden layers of ReLUs.\n",
    "  first_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([card_width*card_height, first_hidden_layer_size]), name='first_hidden_weights')\n",
    "  first_hidden_biases = tf.Variable(\n",
    "    tf.zeros([first_hidden_layer_size]), name='first_hidden_biases')\n",
    "  first_hidden_layer = tf.nn.relu(tf.matmul(tf_training_data, first_hidden_weights) + first_hidden_biases)\n",
    "  second_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([first_hidden_layer_size, second_hidden_layer_size]), name='second_hidden_weights')\n",
    "  second_hidden_biases = tf.Variable(\n",
    "    tf.zeros([second_hidden_layer_size]), name='second_hidden_biases')\n",
    "  second_hidden_layer = tf.nn.relu(tf.matmul(first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  \n",
    "  # Build the output layer.\n",
    "  output_weights = tf.Variable(tf.truncated_normal([second_hidden_layer_size, 3]), name='output_weights')\n",
    "  output_biases = tf.Variable(tf.zeros([3]), name='output_biases')\n",
    "  output_logits = tf.matmul(second_hidden_layer, output_weights) + output_biases\n",
    "  training_estimate = tf.nn.softmax(output_logits)\n",
    "\n",
    "  # Calculate loss and setup the optimizer.\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, tf_training_labels))\n",
    "  l2_regularization = (tf.nn.l2_loss(output_weights) +\n",
    "                       tf.nn.l2_loss(first_hidden_weights) +\n",
    "                       tf.nn.l2_loss(second_hidden_weights))\n",
    "  loss += regularization_factor * l2_regularization\n",
    "  training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Setup validation.  We have to reshape into a \"dense tensor\"\n",
    "  # by, essentially, combining this array of arrays into a true matrix.\n",
    "  tf_validation_pixel_data = tf.constant(\n",
    "    validation_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  validation_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_validation_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  validation_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(validation_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  validation_logits = tf.matmul(validation_second_hidden_layer, output_weights) + output_biases\n",
    "  validation_estimate = tf.nn.softmax(validation_logits)\n",
    "\n",
    "  # Setup the final test run.\n",
    "  tf_test_pixel_data = tf.constant(\n",
    "    test_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  test_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_test_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  test_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(test_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  test_logits = tf.matmul(test_second_hidden_layer, output_weights) + output_biases\n",
    "  test_estimate = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "aside: create a small function to calculate the accuracy of a set of predictions\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, ground_truth):\n",
    "  \"\"\"Determine what proportion of predictions are accurate based on ground truth.\"\"\"\n",
    "  correctness = np.sum(np.argmax(predictions, 1) == np.argmax(ground_truth, 1))\n",
    "  return 100. * correctness / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "and run the optimizer in batches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 -> loss: 6102.55, training accuracy: 37.0%, validation accuracy: 30.9%\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "total_iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  variable_saver = tf.train.Saver()\n",
    "\n",
    "  for iteration in range(total_iterations + 1):\n",
    "    batch_data = random.sample(training_data, batch_size)\n",
    "    batch_input_pixel_data = [pixel_data for _, _, pixel_data in batch_data]\n",
    "    batch_labels = [\n",
    "      all_labels[filename.split('-')[3]] for filename, _, _ in batch_data\n",
    "    ]\n",
    "    batch_training_data = {\n",
    "      tf_training_data: batch_input_pixel_data,\n",
    "      tf_training_labels: batch_labels,\n",
    "    }\n",
    "    _, step_loss, training_predictions = session.run(\n",
    "      [training_step, loss, training_estimate], feed_dict=batch_training_data)\n",
    "  \n",
    "    if (iteration % (total_iterations / 20)) == 0:\n",
    "      training_accuracy = calculate_accuracy(training_predictions, batch_labels)\n",
    "      validation_accuracy = calculate_accuracy(validation_estimate.eval(), validation_labels)\n",
    "      accuracies.append((iteration, training_accuracy, validation_accuracy))\n",
    "      if (iteration % (total_iterations / 10)) == 0:\n",
    "        print 'iteration: %s -> loss: %s, training accuracy: %0.1f%%, validation accuracy: %0.1f%%' % (\n",
    "          iteration, step_loss, training_accuracy, validation_accuracy)\n",
    "  variable_saver.save(session, '/tmp/detect-shape-model.ckpt', latest_filename='detect-shape-checkpoint-list')\n",
    "  print '\\ntest accuracy: %0.1f%%' % calculate_accuracy(test_estimate.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "plot the accuracy vs iteration number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iterations, training_accuracies, validation_accuracies = zip(*accuracies)\n",
    "plt.plot(iterations, training_accuracies, 'r-', label='training')\n",
    "plt.plot(iterations, validation_accuracies, 'g-', label='validation')\n",
    "axes = plt.gca()\n",
    "_ = axes.set_ylim([0, 110])\n",
    "_ = plt.xlabel('iteration')\n",
    "_ = plt.ylabel('accuracy')\n",
    "_ = plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
