{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "We'll use tensorflow to predict the type of shape in each image.\n",
    "\n",
    "We should already have `.npy` files in `greyscale-data`.  We'll first load data into various structures for later.  This cell mainly splits the data into training, validation and test folds.  To make the splits I'm hashing filenames and then sorting those hashes alphabetically -- this mixes up the images, but makes the mixing deterministic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "input_directory = 'greyscaled-data'\n",
    "\n",
    "# Load all the data into an array.\n",
    "# Each element is a tuple: (filename, numpy data).\n",
    "# The filename structure is \"<number>-<color>-<texture>-<shape>-<rotation>.png\"\n",
    "all_data = [\n",
    "  (f, np.load(os.path.join(input_directory, f))) for f in os.listdir(input_directory)\n",
    "]\n",
    "\n",
    "# Hash the filename and sort the hashes alphabetically.\n",
    "all_data_with_hashes = [\n",
    "  (filename, hashlib.md5(filename).hexdigest(), data) for filename, data in all_data\n",
    "]\n",
    "all_data_sorted = sorted(all_data_with_hashes, key=lambda element: element[1])\n",
    "\n",
    "# Save 20% of the data for testing (the final, one-shot evaluation of performance).\n",
    "split_index = int(0.2 * len(all_data_sorted))\n",
    "test_data = all_data_sorted[0:split_index]\n",
    "remaining_data = all_data_sorted[split_index:]\n",
    "\n",
    "# Now save 20% of the remaining data for validation.\n",
    "split_index = int(0.2 * len(remaining_data))\n",
    "validation_data = remaining_data[0:split_index]\n",
    "training_data = remaining_data[split_index:]\n",
    "\n",
    "# For convenience, get all the pixel data into separate arrays.\n",
    "training_pixel_data = [pixel_data for _, _, pixel_data in training_data]\n",
    "validation_pixel_data = np.array([pixel_data for _, _, pixel_data in validation_data])\n",
    "test_pixel_data = np.array([pixel_data for _, _, pixel_data in test_data])\n",
    "\n",
    "# Each filename, in its text, has an embedded type of shape.\n",
    "# As in, \"2-red-empty-oval-45.npy\"\n",
    "# We need to convert those classes (the output ground truth) into label arrays.\n",
    "all_labels = {\n",
    "  'oval': [1., 0., 0.],\n",
    "  'diamond': [0., 1., 0.],\n",
    "  'bean': [0., 0., 1.],\n",
    "}\n",
    "training_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in training_data\n",
    "]\n",
    "validation_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in validation_data\n",
    "]\n",
    "test_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _, _ in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "setup tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "learning_rate = 0.0000005\n",
    "regularization_factor = 1e-4\n",
    "card_width, card_height = 150, 150\n",
    "first_hidden_layer_size, second_hidden_layer_size = 1024, 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Setup the training steps.\n",
    "  tf_training_data = tf.placeholder(tf.float32, shape=[None, card_width*card_height*card_channels])\n",
    "  tf_training_labels = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "  \n",
    "  # Create hidden layers of ReLUs.\n",
    "  first_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([card_width*card_height*card_channels, first_hidden_layer_size]), name='first_hidden_weights')\n",
    "  first_hidden_biases = tf.Variable(\n",
    "    tf.zeros([first_hidden_layer_size]), name='first_hidden_biases')\n",
    "  first_hidden_layer = tf.nn.relu(tf.matmul(tf_training_data, first_hidden_weights) + first_hidden_biases)\n",
    "  second_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([first_hidden_layer_size, second_hidden_layer_size]), name='second_hidden_weights')\n",
    "  second_hidden_biases = tf.Variable(\n",
    "    tf.zeros([second_hidden_layer_size]), name='second_hidden_biases')\n",
    "  second_hidden_layer = tf.nn.relu(tf.matmul(first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  \n",
    "  # Build the output layer.\n",
    "  output_weights = tf.Variable(tf.truncated_normal([second_hidden_layer_size, 3]), name='output_weights')\n",
    "  output_biases = tf.Variable(tf.zeros([3]), name='output_biases')\n",
    "  output_logits = tf.matmul(second_hidden_layer, output_weights) + output_biases\n",
    "  training_estimate = tf.nn.softmax(output_logits)\n",
    "\n",
    "  # Calculate loss and setup the optimizer.\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, tf_training_labels))\n",
    "  l2_regularization = (tf.nn.l2_loss(output_weights) +\n",
    "                       tf.nn.l2_loss(first_hidden_weights) +\n",
    "                       tf.nn.l2_loss(second_hidden_weights))\n",
    "  loss += regularization_factor * l2_regularization\n",
    "  training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Setup validation.  We have to reshape into a \"dense tensor\"\n",
    "  # by, essentially, combining this array of arrays into a true matrix.\n",
    "  tf_validation_pixel_data = tf.constant(\n",
    "    validation_pixel_data.reshape((-1, card_width*card_height*card_channels)).astype(np.float32))\n",
    "  validation_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_validation_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  validation_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(validation_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  validation_logits = tf.matmul(validation_second_hidden_layer, output_weights) + output_biases\n",
    "  validation_estimate = tf.nn.softmax(validation_logits)\n",
    "\n",
    "  # Setup the final test run.\n",
    "  tf_test_pixel_data = tf.constant(\n",
    "    test_pixel_data.reshape((-1, card_width*card_height*card_channels)).astype(np.float32))\n",
    "  test_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_test_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  test_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(test_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  test_logits = tf.matmul(test_second_hidden_layer, output_weights) + output_biases\n",
    "  test_estimate = tf.nn.softmax(test_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
