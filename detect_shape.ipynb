{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "We'll use tensorflow to predict the type of shape in each image: oval, diamond or bean.\n",
    "\n",
    "We should already have `.npy` files in `greyscale-data`.  We'll first load data into various structures for later.  This cell mainly splits the data into training, validation and test folds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_directory = 'greyscaled-data'\n",
    "\n",
    "# Load all the data into an array.  Each element is a tuple: (filename, numpy data).\n",
    "# The filename structure is \"<number>-<color>-<texture>-<shape>-<rotation>.png\"\n",
    "# We'll sort the data first so the later shuffle is consistent.\n",
    "all_data = [\n",
    "  (f, np.load(os.path.join(input_directory, f))) for f in os.listdir(input_directory)\n",
    "]\n",
    "all_data_sorted = sorted(all_data, key=lambda element: element[0])\n",
    "random.seed(2)\n",
    "random.shuffle(all_data_sorted)\n",
    "\n",
    "# Save 5% of the data for testing (the final, one-shot evaluation of performance).\n",
    "split_index = int(0.05 * len(all_data_sorted))\n",
    "test_data = all_data_sorted[0:split_index]\n",
    "remaining_data = all_data_sorted[split_index:]\n",
    "\n",
    "# Now save 5% of the remaining data for validation.\n",
    "split_index = int(0.05 * len(remaining_data))\n",
    "validation_data = remaining_data[0:split_index]\n",
    "training_data = remaining_data[split_index:]\n",
    "\n",
    "# For convenience, get all the pixel data into separate arrays.\n",
    "training_pixel_data = [pixel_data for _, pixel_data in training_data]\n",
    "validation_pixel_data = np.array([pixel_data for _, pixel_data in validation_data])\n",
    "test_pixel_data = np.array([pixel_data for _, pixel_data in test_data])\n",
    "\n",
    "# Each filename, in its text, has an embedded type of shape.\n",
    "# As in, \"2-red-empty-oval-45.npy\"\n",
    "# We need to convert those classes (the output ground truth) into label arrays.\n",
    "all_labels = {\n",
    "  'oval': [1., 0., 0.],\n",
    "  'diamond': [0., 1., 0.],\n",
    "  'bean': [0., 0., 1.],\n",
    "}\n",
    "training_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _ in training_data\n",
    "]\n",
    "validation_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _ in validation_data\n",
    "]\n",
    "test_labels = [\n",
    "  all_labels[filename.split('-')[3]] for filename, _ in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "setup tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "learning_rate = 1e-4\n",
    "regularization_factor = 1e-4\n",
    "card_width, card_height = 150, 150\n",
    "first_hidden_layer_size, second_hidden_layer_size = 1024, 32\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Setup the training steps.\n",
    "  tf_training_data = tf.placeholder(tf.float32, shape=[None, card_width*card_height])\n",
    "  tf_training_labels = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "  \n",
    "  # Create hidden layers of ReLUs.\n",
    "  first_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([card_width*card_height, first_hidden_layer_size]), name='first_hidden_weights')\n",
    "  first_hidden_biases = tf.Variable(\n",
    "    tf.zeros([first_hidden_layer_size]), name='first_hidden_biases')\n",
    "  first_hidden_layer = tf.nn.relu(tf.matmul(tf_training_data, first_hidden_weights) + first_hidden_biases)\n",
    "  second_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([first_hidden_layer_size, second_hidden_layer_size]), name='second_hidden_weights')\n",
    "  second_hidden_biases = tf.Variable(\n",
    "    tf.zeros([second_hidden_layer_size]), name='second_hidden_biases')\n",
    "  second_hidden_layer = tf.nn.relu(tf.matmul(first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  \n",
    "  # Build the output layer.\n",
    "  output_weights = tf.Variable(tf.truncated_normal([second_hidden_layer_size, 3]), name='output_weights')\n",
    "  output_biases = tf.Variable(tf.zeros([3]), name='output_biases')\n",
    "  output_logits = tf.matmul(second_hidden_layer, output_weights) + output_biases\n",
    "  training_estimate = tf.nn.softmax(output_logits)\n",
    "\n",
    "  # Calculate loss and setup the optimizer.\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, tf_training_labels))\n",
    "  l2_regularization = (tf.nn.l2_loss(output_weights) +\n",
    "                       tf.nn.l2_loss(first_hidden_weights) +\n",
    "                       tf.nn.l2_loss(second_hidden_weights))\n",
    "  loss += regularization_factor * l2_regularization\n",
    "  training_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Setup validation.  We have to reshape into a \"dense tensor\"\n",
    "  # by, essentially, combining this array of arrays into a true matrix.\n",
    "  tf_validation_pixel_data = tf.constant(\n",
    "    validation_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  validation_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_validation_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  validation_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(validation_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  validation_logits = tf.matmul(validation_second_hidden_layer, output_weights) + output_biases\n",
    "  validation_estimate = tf.nn.softmax(validation_logits)\n",
    "\n",
    "  # Setup the final test run.\n",
    "  tf_test_pixel_data = tf.constant(\n",
    "    test_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  test_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_test_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  test_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(test_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  test_logits = tf.matmul(test_second_hidden_layer, output_weights) + output_biases\n",
    "  test_estimate = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "aside: create a small function to calculate the accuracy of a set of predictions\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, ground_truth):\n",
    "  \"\"\"Determine what proportion of predictions are accurate based on ground truth.\"\"\"\n",
    "  correctness = np.sum(np.argmax(predictions, 1) == np.argmax(ground_truth, 1))\n",
    "  return 100. * correctness / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "and run the optimizer in batches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 -> loss: 1.51611e+06, training accuracy: 31.0%, validation accuracy: 32.3%\n",
      "iteration: 2 -> loss: 857718.0, training accuracy: 35.0%, validation accuracy: 34.9%\n",
      "iteration: 4 -> loss: 414012.0, training accuracy: 41.0%, validation accuracy: 34.2%\n",
      "iteration: 6 -> loss: 320769.0, training accuracy: 46.0%, validation accuracy: 34.9%\n",
      "iteration: 8 -> loss: 364294.0, training accuracy: 32.0%, validation accuracy: 37.1%\n",
      "iteration: 10 -> loss: 225301.0, training accuracy: 40.0%, validation accuracy: 37.2%\n",
      "iteration: 12 -> loss: 215529.0, training accuracy: 33.0%, validation accuracy: 35.9%\n",
      "iteration: 14 -> loss: 268532.0, training accuracy: 38.0%, validation accuracy: 35.7%\n",
      "iteration: 16 -> loss: 216512.0, training accuracy: 35.0%, validation accuracy: 36.7%\n",
      "iteration: 18 -> loss: 142917.0, training accuracy: 46.0%, validation accuracy: 38.1%\n",
      "iteration: 20 -> loss: 159414.0, training accuracy: 40.0%, validation accuracy: 39.0%\n",
      "iteration: 22 -> loss: 173454.0, training accuracy: 38.0%, validation accuracy: 39.7%\n",
      "iteration: 24 -> loss: 163796.0, training accuracy: 42.0%, validation accuracy: 40.3%\n",
      "iteration: 26 -> loss: 107683.0, training accuracy: 48.0%, validation accuracy: 41.1%\n",
      "iteration: 28 -> loss: 112040.0, training accuracy: 41.0%, validation accuracy: 39.9%\n",
      "iteration: 30 -> loss: 151266.0, training accuracy: 34.0%, validation accuracy: 40.3%\n",
      "iteration: 32 -> loss: 124671.0, training accuracy: 36.0%, validation accuracy: 41.3%\n",
      "iteration: 34 -> loss: 114384.0, training accuracy: 48.0%, validation accuracy: 41.6%\n",
      "iteration: 36 -> loss: 98209.8, training accuracy: 40.0%, validation accuracy: 41.9%\n",
      "iteration: 38 -> loss: 115086.0, training accuracy: 49.0%, validation accuracy: 42.1%\n",
      "iteration: 40 -> loss: 102560.0, training accuracy: 42.0%, validation accuracy: 42.4%\n",
      "iteration: 42 -> loss: 100726.0, training accuracy: 37.0%, validation accuracy: 42.8%\n",
      "iteration: 44 -> loss: 73701.9, training accuracy: 48.0%, validation accuracy: 43.3%\n",
      "iteration: 46 -> loss: 79482.7, training accuracy: 51.0%, validation accuracy: 43.5%\n",
      "iteration: 48 -> loss: 77562.3, training accuracy: 43.0%, validation accuracy: 44.2%\n",
      "iteration: 50 -> loss: 103441.0, training accuracy: 40.0%, validation accuracy: 44.5%\n",
      "iteration: 52 -> loss: 97659.2, training accuracy: 41.0%, validation accuracy: 43.8%\n",
      "iteration: 54 -> loss: 53680.7, training accuracy: 53.0%, validation accuracy: 44.5%\n",
      "iteration: 56 -> loss: 75177.4, training accuracy: 46.0%, validation accuracy: 44.0%\n",
      "iteration: 58 -> loss: 49865.1, training accuracy: 50.0%, validation accuracy: 43.0%\n",
      "iteration: 60 -> loss: 85876.6, training accuracy: 45.0%, validation accuracy: 43.0%\n",
      "iteration: 62 -> loss: 67502.5, training accuracy: 44.0%, validation accuracy: 44.5%\n",
      "iteration: 64 -> loss: 67058.1, training accuracy: 43.0%, validation accuracy: 47.0%\n",
      "iteration: 66 -> loss: 44300.1, training accuracy: 60.0%, validation accuracy: 46.4%\n",
      "iteration: 68 -> loss: 71764.9, training accuracy: 45.0%, validation accuracy: 46.3%\n",
      "iteration: 70 -> loss: 67327.0, training accuracy: 48.0%, validation accuracy: 48.0%\n",
      "iteration: 72 -> loss: 65433.6, training accuracy: 44.0%, validation accuracy: 47.7%\n",
      "iteration: 74 -> loss: 80967.4, training accuracy: 39.0%, validation accuracy: 47.2%\n",
      "iteration: 76 -> loss: 67329.7, training accuracy: 42.0%, validation accuracy: 47.2%\n",
      "iteration: 78 -> loss: 73681.1, training accuracy: 45.0%, validation accuracy: 46.7%\n",
      "iteration: 80 -> loss: 64794.6, training accuracy: 48.0%, validation accuracy: 46.9%\n",
      "iteration: 82 -> loss: 65014.6, training accuracy: 56.0%, validation accuracy: 48.5%\n",
      "iteration: 84 -> loss: 75519.6, training accuracy: 48.0%, validation accuracy: 46.6%\n",
      "iteration: 86 -> loss: 66944.9, training accuracy: 40.0%, validation accuracy: 44.8%\n",
      "iteration: 88 -> loss: 56105.5, training accuracy: 50.0%, validation accuracy: 52.1%\n",
      "iteration: 90 -> loss: 62744.5, training accuracy: 54.0%, validation accuracy: 51.8%\n",
      "iteration: 92 -> loss: 48917.7, training accuracy: 56.0%, validation accuracy: 52.6%\n",
      "iteration: 94 -> loss: 62065.8, training accuracy: 46.0%, validation accuracy: 51.3%\n",
      "iteration: 96 -> loss: 57288.3, training accuracy: 48.0%, validation accuracy: 52.1%\n",
      "iteration: 98 -> loss: 44392.1, training accuracy: 56.0%, validation accuracy: 51.2%\n",
      "iteration: 100 -> loss: 39816.1, training accuracy: 60.0%, validation accuracy: 48.8%\n",
      "iteration: 102 -> loss: 49858.3, training accuracy: 48.0%, validation accuracy: 45.8%\n",
      "iteration: 104 -> loss: 64589.1, training accuracy: 42.0%, validation accuracy: 48.6%\n",
      "iteration: 106 -> loss: 64558.4, training accuracy: 43.0%, validation accuracy: 51.3%\n",
      "iteration: 108 -> loss: 45825.8, training accuracy: 56.0%, validation accuracy: 52.4%\n",
      "iteration: 110 -> loss: 42819.7, training accuracy: 54.0%, validation accuracy: 52.1%\n",
      "iteration: 112 -> loss: 52556.5, training accuracy: 59.0%, validation accuracy: 51.3%\n",
      "iteration: 114 -> loss: 40812.9, training accuracy: 48.0%, validation accuracy: 51.9%\n",
      "iteration: 116 -> loss: 47192.9, training accuracy: 53.0%, validation accuracy: 53.2%\n",
      "iteration: 118 -> loss: 56068.7, training accuracy: 49.0%, validation accuracy: 53.5%\n",
      "iteration: 120 -> loss: 36677.5, training accuracy: 55.0%, validation accuracy: 51.7%\n",
      "iteration: 122 -> loss: 33833.7, training accuracy: 65.0%, validation accuracy: 52.1%\n",
      "iteration: 124 -> loss: 42739.8, training accuracy: 51.0%, validation accuracy: 53.0%\n",
      "iteration: 126 -> loss: 54786.2, training accuracy: 49.0%, validation accuracy: 54.2%\n",
      "iteration: 128 -> loss: 44994.8, training accuracy: 54.0%, validation accuracy: 53.5%\n",
      "iteration: 130 -> loss: 55634.6, training accuracy: 52.0%, validation accuracy: 54.8%\n",
      "iteration: 132 -> loss: 54181.9, training accuracy: 52.0%, validation accuracy: 54.5%\n",
      "iteration: 134 -> loss: 37483.2, training accuracy: 57.0%, validation accuracy: 55.4%\n",
      "iteration: 136 -> loss: 44623.0, training accuracy: 44.0%, validation accuracy: 52.9%\n",
      "iteration: 138 -> loss: 21253.7, training accuracy: 70.0%, validation accuracy: 55.0%\n",
      "iteration: 140 -> loss: 37999.3, training accuracy: 57.0%, validation accuracy: 52.9%\n",
      "iteration: 142 -> loss: 35651.2, training accuracy: 54.0%, validation accuracy: 53.8%\n",
      "iteration: 144 -> loss: 50275.9, training accuracy: 53.0%, validation accuracy: 53.7%\n",
      "iteration: 146 -> loss: 34648.1, training accuracy: 66.0%, validation accuracy: 56.9%\n",
      "iteration: 148 -> loss: 36713.4, training accuracy: 60.0%, validation accuracy: 56.4%\n",
      "iteration: 150 -> loss: 44692.2, training accuracy: 51.0%, validation accuracy: 53.8%\n",
      "iteration: 152 -> loss: 43315.2, training accuracy: 54.0%, validation accuracy: 53.6%\n",
      "iteration: 154 -> loss: 46630.5, training accuracy: 51.0%, validation accuracy: 56.8%\n",
      "iteration: 156 -> loss: 50605.3, training accuracy: 57.0%, validation accuracy: 56.2%\n",
      "iteration: 158 -> loss: 30627.8, training accuracy: 61.0%, validation accuracy: 55.6%\n",
      "iteration: 160 -> loss: 36616.8, training accuracy: 54.0%, validation accuracy: 57.4%\n",
      "iteration: 162 -> loss: 22798.9, training accuracy: 65.0%, validation accuracy: 56.4%\n",
      "iteration: 164 -> loss: 43579.9, training accuracy: 57.0%, validation accuracy: 58.0%\n",
      "iteration: 166 -> loss: 28133.5, training accuracy: 67.0%, validation accuracy: 53.2%\n",
      "iteration: 168 -> loss: 55816.7, training accuracy: 53.0%, validation accuracy: 56.3%\n",
      "iteration: 170 -> loss: 38625.3, training accuracy: 58.0%, validation accuracy: 55.6%\n",
      "iteration: 172 -> loss: 49630.4, training accuracy: 54.0%, validation accuracy: 59.0%\n",
      "iteration: 174 -> loss: 33595.6, training accuracy: 59.0%, validation accuracy: 55.1%\n",
      "iteration: 176 -> loss: 28272.8, training accuracy: 60.0%, validation accuracy: 58.4%\n",
      "iteration: 178 -> loss: 35323.9, training accuracy: 57.0%, validation accuracy: 56.8%\n",
      "iteration: 180 -> loss: 26567.2, training accuracy: 65.0%, validation accuracy: 59.1%\n",
      "iteration: 182 -> loss: 21273.8, training accuracy: 64.0%, validation accuracy: 58.1%\n",
      "iteration: 184 -> loss: 49083.8, training accuracy: 67.0%, validation accuracy: 61.7%\n",
      "iteration: 186 -> loss: 38496.3, training accuracy: 61.0%, validation accuracy: 59.7%\n",
      "iteration: 188 -> loss: 33980.6, training accuracy: 59.0%, validation accuracy: 61.0%\n",
      "iteration: 190 -> loss: 36557.2, training accuracy: 57.0%, validation accuracy: 59.3%\n",
      "iteration: 192 -> loss: 47161.0, training accuracy: 68.0%, validation accuracy: 60.8%\n",
      "iteration: 194 -> loss: 30022.3, training accuracy: 62.0%, validation accuracy: 58.3%\n",
      "iteration: 196 -> loss: 32614.3, training accuracy: 59.0%, validation accuracy: 57.9%\n",
      "iteration: 198 -> loss: 38182.3, training accuracy: 57.0%, validation accuracy: 55.8%\n",
      "iteration: 200 -> loss: 31752.2, training accuracy: 63.0%, validation accuracy: 62.6%\n",
      "iteration: 202 -> loss: 18027.9, training accuracy: 68.0%, validation accuracy: 62.8%\n",
      "iteration: 204 -> loss: 19512.4, training accuracy: 70.0%, validation accuracy: 63.6%\n",
      "iteration: 206 -> loss: 26546.2, training accuracy: 64.0%, validation accuracy: 63.4%\n",
      "iteration: 208 -> loss: 30606.0, training accuracy: 63.0%, validation accuracy: 63.7%\n",
      "iteration: 210 -> loss: 26014.2, training accuracy: 65.0%, validation accuracy: 61.1%\n",
      "iteration: 212 -> loss: 21113.3, training accuracy: 76.0%, validation accuracy: 62.4%\n",
      "iteration: 214 -> loss: 24204.3, training accuracy: 60.0%, validation accuracy: 61.4%\n",
      "iteration: 216 -> loss: 21952.1, training accuracy: 63.0%, validation accuracy: 64.8%\n",
      "iteration: 218 -> loss: 21057.3, training accuracy: 72.0%, validation accuracy: 63.2%\n",
      "iteration: 220 -> loss: 26438.1, training accuracy: 68.0%, validation accuracy: 64.7%\n",
      "iteration: 222 -> loss: 28215.9, training accuracy: 60.0%, validation accuracy: 63.2%\n",
      "iteration: 224 -> loss: 24334.9, training accuracy: 63.0%, validation accuracy: 64.2%\n",
      "iteration: 226 -> loss: 27609.2, training accuracy: 62.0%, validation accuracy: 65.0%\n",
      "iteration: 228 -> loss: 21624.6, training accuracy: 68.0%, validation accuracy: 63.9%\n",
      "iteration: 230 -> loss: 29375.5, training accuracy: 71.0%, validation accuracy: 66.4%\n",
      "iteration: 232 -> loss: 29260.0, training accuracy: 68.0%, validation accuracy: 66.6%\n",
      "iteration: 234 -> loss: 30885.5, training accuracy: 68.0%, validation accuracy: 66.4%\n",
      "iteration: 236 -> loss: 26724.2, training accuracy: 71.0%, validation accuracy: 67.5%\n",
      "iteration: 238 -> loss: 24872.0, training accuracy: 71.0%, validation accuracy: 63.1%\n",
      "iteration: 240 -> loss: 16060.4, training accuracy: 74.0%, validation accuracy: 65.7%\n",
      "iteration: 242 -> loss: 22388.1, training accuracy: 63.0%, validation accuracy: 60.6%\n",
      "iteration: 244 -> loss: 24036.5, training accuracy: 60.0%, validation accuracy: 65.6%\n",
      "iteration: 246 -> loss: 20053.0, training accuracy: 60.0%, validation accuracy: 65.4%\n",
      "iteration: 248 -> loss: 15529.9, training accuracy: 68.0%, validation accuracy: 67.1%\n",
      "iteration: 250 -> loss: 26607.9, training accuracy: 61.0%, validation accuracy: 68.9%\n",
      "iteration: 252 -> loss: 15923.1, training accuracy: 74.0%, validation accuracy: 67.1%\n",
      "iteration: 254 -> loss: 17150.4, training accuracy: 65.0%, validation accuracy: 68.2%\n",
      "iteration: 256 -> loss: 18281.6, training accuracy: 73.0%, validation accuracy: 68.0%\n",
      "iteration: 258 -> loss: 24897.2, training accuracy: 61.0%, validation accuracy: 67.1%\n",
      "iteration: 260 -> loss: 19500.1, training accuracy: 67.0%, validation accuracy: 66.5%\n",
      "iteration: 262 -> loss: 25489.3, training accuracy: 65.0%, validation accuracy: 66.5%\n",
      "iteration: 264 -> loss: 21737.1, training accuracy: 64.0%, validation accuracy: 65.4%\n",
      "iteration: 266 -> loss: 14707.9, training accuracy: 65.0%, validation accuracy: 68.2%\n",
      "iteration: 268 -> loss: 26000.8, training accuracy: 62.0%, validation accuracy: 67.6%\n",
      "iteration: 270 -> loss: 28050.0, training accuracy: 58.0%, validation accuracy: 69.3%\n",
      "iteration: 272 -> loss: 17570.4, training accuracy: 69.0%, validation accuracy: 66.9%\n",
      "iteration: 274 -> loss: 11135.2, training accuracy: 68.0%, validation accuracy: 67.6%\n",
      "iteration: 276 -> loss: 21816.7, training accuracy: 69.0%, validation accuracy: 66.4%\n",
      "iteration: 278 -> loss: 16565.0, training accuracy: 68.0%, validation accuracy: 67.0%\n",
      "iteration: 280 -> loss: 25673.8, training accuracy: 67.0%, validation accuracy: 66.9%\n",
      "iteration: 282 -> loss: 18059.3, training accuracy: 70.0%, validation accuracy: 69.2%\n",
      "iteration: 284 -> loss: 29847.5, training accuracy: 67.0%, validation accuracy: 69.0%\n",
      "iteration: 286 -> loss: 18488.5, training accuracy: 74.0%, validation accuracy: 69.7%\n",
      "iteration: 288 -> loss: 18256.8, training accuracy: 69.0%, validation accuracy: 70.1%\n",
      "iteration: 290 -> loss: 7839.03, training accuracy: 74.0%, validation accuracy: 69.0%\n",
      "iteration: 292 -> loss: 29822.7, training accuracy: 62.0%, validation accuracy: 69.3%\n",
      "iteration: 294 -> loss: 24397.3, training accuracy: 70.0%, validation accuracy: 69.8%\n",
      "iteration: 296 -> loss: 28125.9, training accuracy: 58.0%, validation accuracy: 69.4%\n",
      "iteration: 298 -> loss: 19910.7, training accuracy: 68.0%, validation accuracy: 69.3%\n",
      "iteration: 300 -> loss: 15851.4, training accuracy: 73.0%, validation accuracy: 71.4%\n",
      "iteration: 302 -> loss: 14301.9, training accuracy: 69.0%, validation accuracy: 69.2%\n",
      "iteration: 304 -> loss: 19108.0, training accuracy: 65.0%, validation accuracy: 71.3%\n",
      "iteration: 306 -> loss: 11884.8, training accuracy: 72.0%, validation accuracy: 70.1%\n",
      "iteration: 308 -> loss: 25421.0, training accuracy: 61.0%, validation accuracy: 72.2%\n",
      "iteration: 310 -> loss: 18470.7, training accuracy: 65.0%, validation accuracy: 70.6%\n",
      "iteration: 312 -> loss: 23961.0, training accuracy: 67.0%, validation accuracy: 70.7%\n",
      "iteration: 314 -> loss: 21397.0, training accuracy: 64.0%, validation accuracy: 71.0%\n",
      "iteration: 316 -> loss: 23351.5, training accuracy: 73.0%, validation accuracy: 73.8%\n",
      "iteration: 318 -> loss: 13106.6, training accuracy: 79.0%, validation accuracy: 70.6%\n",
      "iteration: 320 -> loss: 20086.8, training accuracy: 74.0%, validation accuracy: 69.5%\n",
      "iteration: 322 -> loss: 18248.9, training accuracy: 77.0%, validation accuracy: 68.9%\n",
      "iteration: 324 -> loss: 10630.9, training accuracy: 77.0%, validation accuracy: 72.1%\n",
      "iteration: 326 -> loss: 22361.0, training accuracy: 67.0%, validation accuracy: 68.3%\n",
      "iteration: 328 -> loss: 18580.8, training accuracy: 75.0%, validation accuracy: 68.5%\n",
      "iteration: 330 -> loss: 25810.2, training accuracy: 65.0%, validation accuracy: 73.4%\n",
      "iteration: 332 -> loss: 12509.1, training accuracy: 69.0%, validation accuracy: 70.4%\n",
      "iteration: 334 -> loss: 12133.3, training accuracy: 77.0%, validation accuracy: 68.9%\n",
      "iteration: 336 -> loss: 10669.9, training accuracy: 72.0%, validation accuracy: 71.1%\n",
      "iteration: 338 -> loss: 23964.1, training accuracy: 66.0%, validation accuracy: 70.7%\n",
      "iteration: 340 -> loss: 15799.3, training accuracy: 69.0%, validation accuracy: 71.4%\n",
      "iteration: 342 -> loss: 18403.3, training accuracy: 63.0%, validation accuracy: 73.4%\n",
      "iteration: 344 -> loss: 15913.0, training accuracy: 78.0%, validation accuracy: 74.3%\n",
      "iteration: 346 -> loss: 17855.6, training accuracy: 74.0%, validation accuracy: 71.7%\n",
      "iteration: 348 -> loss: 18727.8, training accuracy: 66.0%, validation accuracy: 72.2%\n",
      "iteration: 350 -> loss: 9801.33, training accuracy: 76.0%, validation accuracy: 73.9%\n",
      "iteration: 352 -> loss: 23632.2, training accuracy: 68.0%, validation accuracy: 71.0%\n",
      "iteration: 354 -> loss: 9998.82, training accuracy: 80.0%, validation accuracy: 73.8%\n",
      "iteration: 356 -> loss: 17151.1, training accuracy: 72.0%, validation accuracy: 71.2%\n",
      "iteration: 358 -> loss: 14462.4, training accuracy: 73.0%, validation accuracy: 71.0%\n",
      "iteration: 360 -> loss: 16237.8, training accuracy: 72.0%, validation accuracy: 70.1%\n",
      "iteration: 362 -> loss: 14092.7, training accuracy: 76.0%, validation accuracy: 71.4%\n",
      "iteration: 364 -> loss: 16831.6, training accuracy: 73.0%, validation accuracy: 71.8%\n",
      "iteration: 366 -> loss: 11142.7, training accuracy: 73.0%, validation accuracy: 73.0%\n",
      "iteration: 368 -> loss: 21276.1, training accuracy: 66.0%, validation accuracy: 73.1%\n",
      "iteration: 370 -> loss: 10431.0, training accuracy: 83.0%, validation accuracy: 73.5%\n",
      "iteration: 372 -> loss: 11638.1, training accuracy: 73.0%, validation accuracy: 74.0%\n",
      "iteration: 374 -> loss: 10056.0, training accuracy: 80.0%, validation accuracy: 70.6%\n",
      "iteration: 376 -> loss: 16870.8, training accuracy: 68.0%, validation accuracy: 73.8%\n",
      "iteration: 378 -> loss: 13189.9, training accuracy: 82.0%, validation accuracy: 75.0%\n",
      "iteration: 380 -> loss: 10682.9, training accuracy: 78.0%, validation accuracy: 74.0%\n",
      "iteration: 382 -> loss: 12855.4, training accuracy: 82.0%, validation accuracy: 75.3%\n",
      "iteration: 384 -> loss: 10437.6, training accuracy: 78.0%, validation accuracy: 72.8%\n",
      "iteration: 386 -> loss: 12881.7, training accuracy: 70.0%, validation accuracy: 74.9%\n",
      "iteration: 388 -> loss: 9445.06, training accuracy: 76.0%, validation accuracy: 72.9%\n",
      "iteration: 390 -> loss: 14785.2, training accuracy: 74.0%, validation accuracy: 74.6%\n",
      "iteration: 392 -> loss: 19579.6, training accuracy: 73.0%, validation accuracy: 73.0%\n",
      "iteration: 394 -> loss: 18934.0, training accuracy: 67.0%, validation accuracy: 73.5%\n",
      "iteration: 396 -> loss: 21955.1, training accuracy: 70.0%, validation accuracy: 75.6%\n",
      "iteration: 398 -> loss: 12065.8, training accuracy: 80.0%, validation accuracy: 73.4%\n",
      "iteration: 400 -> loss: 13691.2, training accuracy: 73.0%, validation accuracy: 75.9%\n",
      "iteration: 402 -> loss: 11901.0, training accuracy: 78.0%, validation accuracy: 71.7%\n",
      "iteration: 404 -> loss: 21492.3, training accuracy: 73.0%, validation accuracy: 77.0%\n",
      "iteration: 406 -> loss: 10915.8, training accuracy: 78.0%, validation accuracy: 71.6%\n",
      "iteration: 408 -> loss: 25386.1, training accuracy: 72.0%, validation accuracy: 76.2%\n",
      "iteration: 410 -> loss: 13166.1, training accuracy: 77.0%, validation accuracy: 73.8%\n",
      "iteration: 412 -> loss: 14693.4, training accuracy: 74.0%, validation accuracy: 75.9%\n",
      "iteration: 414 -> loss: 7311.7, training accuracy: 79.0%, validation accuracy: 73.8%\n",
      "iteration: 416 -> loss: 14750.2, training accuracy: 77.0%, validation accuracy: 75.5%\n",
      "iteration: 418 -> loss: 12920.4, training accuracy: 74.0%, validation accuracy: 77.6%\n",
      "iteration: 420 -> loss: 18974.9, training accuracy: 76.0%, validation accuracy: 75.0%\n",
      "iteration: 422 -> loss: 10329.7, training accuracy: 74.0%, validation accuracy: 76.1%\n",
      "iteration: 424 -> loss: 13714.8, training accuracy: 79.0%, validation accuracy: 79.2%\n",
      "iteration: 426 -> loss: 14520.5, training accuracy: 77.0%, validation accuracy: 78.1%\n",
      "iteration: 428 -> loss: 8657.62, training accuracy: 81.0%, validation accuracy: 78.2%\n",
      "iteration: 430 -> loss: 14295.0, training accuracy: 81.0%, validation accuracy: 77.9%\n",
      "iteration: 432 -> loss: 10600.4, training accuracy: 75.0%, validation accuracy: 77.4%\n",
      "iteration: 434 -> loss: 11396.8, training accuracy: 78.0%, validation accuracy: 78.7%\n",
      "iteration: 436 -> loss: 10665.3, training accuracy: 83.0%, validation accuracy: 78.7%\n",
      "iteration: 438 -> loss: 8027.22, training accuracy: 81.0%, validation accuracy: 79.0%\n",
      "iteration: 440 -> loss: 10796.3, training accuracy: 84.0%, validation accuracy: 79.4%\n",
      "iteration: 442 -> loss: 7634.3, training accuracy: 81.0%, validation accuracy: 78.5%\n",
      "iteration: 444 -> loss: 12250.3, training accuracy: 74.0%, validation accuracy: 78.2%\n",
      "iteration: 446 -> loss: 9887.32, training accuracy: 78.0%, validation accuracy: 77.9%\n",
      "iteration: 448 -> loss: 14309.0, training accuracy: 78.0%, validation accuracy: 77.1%\n",
      "iteration: 450 -> loss: 10454.7, training accuracy: 82.0%, validation accuracy: 80.4%\n",
      "iteration: 452 -> loss: 13766.9, training accuracy: 69.0%, validation accuracy: 76.2%\n",
      "iteration: 454 -> loss: 10041.7, training accuracy: 82.0%, validation accuracy: 75.8%\n",
      "iteration: 456 -> loss: 14064.7, training accuracy: 72.0%, validation accuracy: 76.7%\n",
      "iteration: 458 -> loss: 9747.46, training accuracy: 78.0%, validation accuracy: 75.0%\n",
      "iteration: 460 -> loss: 16304.7, training accuracy: 78.0%, validation accuracy: 77.6%\n",
      "iteration: 462 -> loss: 15053.5, training accuracy: 73.0%, validation accuracy: 75.3%\n",
      "iteration: 464 -> loss: 26248.8, training accuracy: 72.0%, validation accuracy: 74.1%\n",
      "iteration: 466 -> loss: 12048.5, training accuracy: 74.0%, validation accuracy: 77.0%\n",
      "iteration: 468 -> loss: 16382.8, training accuracy: 77.0%, validation accuracy: 78.0%\n",
      "iteration: 470 -> loss: 6832.75, training accuracy: 83.0%, validation accuracy: 80.4%\n",
      "iteration: 472 -> loss: 8500.37, training accuracy: 85.0%, validation accuracy: 77.9%\n",
      "iteration: 474 -> loss: 10579.3, training accuracy: 80.0%, validation accuracy: 77.4%\n",
      "iteration: 476 -> loss: 8756.24, training accuracy: 84.0%, validation accuracy: 78.4%\n",
      "iteration: 478 -> loss: 16240.8, training accuracy: 75.0%, validation accuracy: 79.2%\n",
      "iteration: 480 -> loss: 10726.6, training accuracy: 84.0%, validation accuracy: 76.7%\n",
      "iteration: 482 -> loss: 12461.2, training accuracy: 75.0%, validation accuracy: 75.7%\n",
      "iteration: 484 -> loss: 7736.26, training accuracy: 79.0%, validation accuracy: 78.0%\n",
      "iteration: 486 -> loss: 13022.8, training accuracy: 72.0%, validation accuracy: 77.1%\n",
      "iteration: 488 -> loss: 14343.5, training accuracy: 79.0%, validation accuracy: 79.2%\n",
      "iteration: 490 -> loss: 6142.47, training accuracy: 83.0%, validation accuracy: 77.0%\n",
      "iteration: 492 -> loss: 7684.42, training accuracy: 84.0%, validation accuracy: 76.1%\n",
      "iteration: 494 -> loss: 15423.5, training accuracy: 78.0%, validation accuracy: 78.4%\n",
      "iteration: 496 -> loss: 12721.4, training accuracy: 79.0%, validation accuracy: 80.9%\n",
      "iteration: 498 -> loss: 16379.7, training accuracy: 78.0%, validation accuracy: 77.7%\n",
      "iteration: 500 -> loss: 10817.0, training accuracy: 75.0%, validation accuracy: 80.1%\n",
      "iteration: 502 -> loss: 13408.1, training accuracy: 81.0%, validation accuracy: 79.1%\n",
      "iteration: 504 -> loss: 10724.7, training accuracy: 77.0%, validation accuracy: 78.5%\n",
      "iteration: 506 -> loss: 15001.1, training accuracy: 72.0%, validation accuracy: 76.0%\n",
      "iteration: 508 -> loss: 13902.8, training accuracy: 78.0%, validation accuracy: 79.4%\n",
      "iteration: 510 -> loss: 6087.16, training accuracy: 82.0%, validation accuracy: 77.3%\n",
      "iteration: 512 -> loss: 7313.1, training accuracy: 86.0%, validation accuracy: 76.5%\n",
      "iteration: 514 -> loss: 7842.17, training accuracy: 81.0%, validation accuracy: 79.6%\n",
      "iteration: 516 -> loss: 13079.5, training accuracy: 79.0%, validation accuracy: 80.7%\n",
      "iteration: 518 -> loss: 8167.67, training accuracy: 87.0%, validation accuracy: 69.7%\n",
      "iteration: 520 -> loss: 16031.5, training accuracy: 72.0%, validation accuracy: 78.9%\n",
      "iteration: 522 -> loss: 11255.5, training accuracy: 78.0%, validation accuracy: 77.8%\n",
      "iteration: 524 -> loss: 9603.12, training accuracy: 77.0%, validation accuracy: 80.2%\n",
      "iteration: 526 -> loss: 8860.46, training accuracy: 79.0%, validation accuracy: 77.2%\n",
      "iteration: 528 -> loss: 15261.2, training accuracy: 81.0%, validation accuracy: 79.5%\n",
      "iteration: 530 -> loss: 10942.3, training accuracy: 79.0%, validation accuracy: 78.6%\n",
      "iteration: 532 -> loss: 12400.7, training accuracy: 79.0%, validation accuracy: 77.2%\n",
      "iteration: 534 -> loss: 9032.26, training accuracy: 84.0%, validation accuracy: 81.9%\n",
      "iteration: 536 -> loss: 11868.4, training accuracy: 79.0%, validation accuracy: 80.6%\n",
      "iteration: 538 -> loss: 4915.95, training accuracy: 84.0%, validation accuracy: 79.7%\n",
      "iteration: 540 -> loss: 7207.91, training accuracy: 83.0%, validation accuracy: 79.2%\n",
      "iteration: 542 -> loss: 8071.53, training accuracy: 80.0%, validation accuracy: 76.1%\n",
      "iteration: 544 -> loss: 8150.47, training accuracy: 81.0%, validation accuracy: 80.7%\n",
      "iteration: 546 -> loss: 10374.4, training accuracy: 77.0%, validation accuracy: 79.3%\n",
      "iteration: 548 -> loss: 7041.41, training accuracy: 91.0%, validation accuracy: 81.9%\n",
      "iteration: 550 -> loss: 5336.94, training accuracy: 91.0%, validation accuracy: 81.1%\n",
      "iteration: 552 -> loss: 10033.4, training accuracy: 83.0%, validation accuracy: 80.0%\n",
      "iteration: 554 -> loss: 13422.8, training accuracy: 80.0%, validation accuracy: 81.3%\n",
      "iteration: 556 -> loss: 6412.22, training accuracy: 83.0%, validation accuracy: 80.1%\n",
      "iteration: 558 -> loss: 8660.94, training accuracy: 75.0%, validation accuracy: 80.2%\n",
      "iteration: 560 -> loss: 6632.08, training accuracy: 80.0%, validation accuracy: 80.0%\n",
      "iteration: 562 -> loss: 6062.96, training accuracy: 85.0%, validation accuracy: 79.3%\n",
      "iteration: 564 -> loss: 11384.5, training accuracy: 72.0%, validation accuracy: 82.0%\n",
      "iteration: 566 -> loss: 6587.58, training accuracy: 86.0%, validation accuracy: 78.5%\n",
      "iteration: 568 -> loss: 12166.5, training accuracy: 74.0%, validation accuracy: 80.3%\n",
      "iteration: 570 -> loss: 8263.34, training accuracy: 84.0%, validation accuracy: 81.4%\n",
      "iteration: 572 -> loss: 12615.5, training accuracy: 84.0%, validation accuracy: 82.1%\n",
      "iteration: 574 -> loss: 3828.96, training accuracy: 93.0%, validation accuracy: 82.2%\n",
      "iteration: 576 -> loss: 7333.57, training accuracy: 85.0%, validation accuracy: 82.8%\n",
      "iteration: 578 -> loss: 11723.6, training accuracy: 80.0%, validation accuracy: 81.8%\n",
      "iteration: 580 -> loss: 8709.59, training accuracy: 81.0%, validation accuracy: 83.9%\n",
      "iteration: 582 -> loss: 3982.29, training accuracy: 87.0%, validation accuracy: 81.4%\n",
      "iteration: 584 -> loss: 6863.68, training accuracy: 86.0%, validation accuracy: 83.9%\n",
      "iteration: 586 -> loss: 8804.36, training accuracy: 79.0%, validation accuracy: 82.5%\n",
      "iteration: 588 -> loss: 7364.71, training accuracy: 84.0%, validation accuracy: 83.1%\n",
      "iteration: 590 -> loss: 13021.3, training accuracy: 78.0%, validation accuracy: 79.9%\n",
      "iteration: 592 -> loss: 9124.61, training accuracy: 78.0%, validation accuracy: 81.6%\n",
      "iteration: 594 -> loss: 15937.7, training accuracy: 80.0%, validation accuracy: 81.3%\n",
      "iteration: 596 -> loss: 9696.78, training accuracy: 78.0%, validation accuracy: 82.9%\n",
      "iteration: 598 -> loss: 5236.13, training accuracy: 82.0%, validation accuracy: 80.6%\n",
      "iteration: 600 -> loss: 10513.2, training accuracy: 86.0%, validation accuracy: 83.1%\n",
      "iteration: 602 -> loss: 8554.6, training accuracy: 81.0%, validation accuracy: 79.7%\n",
      "iteration: 604 -> loss: 9245.91, training accuracy: 85.0%, validation accuracy: 82.0%\n",
      "iteration: 606 -> loss: 10217.7, training accuracy: 79.0%, validation accuracy: 79.7%\n",
      "iteration: 608 -> loss: 7830.58, training accuracy: 88.0%, validation accuracy: 82.5%\n",
      "iteration: 610 -> loss: 10682.4, training accuracy: 84.0%, validation accuracy: 82.1%\n",
      "iteration: 612 -> loss: 9166.18, training accuracy: 81.0%, validation accuracy: 83.2%\n",
      "iteration: 614 -> loss: 7126.18, training accuracy: 83.0%, validation accuracy: 84.0%\n",
      "iteration: 616 -> loss: 7349.33, training accuracy: 82.0%, validation accuracy: 83.6%\n",
      "iteration: 618 -> loss: 6857.38, training accuracy: 85.0%, validation accuracy: 83.6%\n",
      "iteration: 620 -> loss: 4569.94, training accuracy: 88.0%, validation accuracy: 80.3%\n",
      "iteration: 622 -> loss: 5720.29, training accuracy: 86.0%, validation accuracy: 84.3%\n",
      "iteration: 624 -> loss: 7245.16, training accuracy: 85.0%, validation accuracy: 84.9%\n",
      "iteration: 626 -> loss: 6159.18, training accuracy: 86.0%, validation accuracy: 80.1%\n",
      "iteration: 628 -> loss: 3490.34, training accuracy: 89.0%, validation accuracy: 85.2%\n",
      "iteration: 630 -> loss: 12303.4, training accuracy: 76.0%, validation accuracy: 81.0%\n",
      "iteration: 632 -> loss: 4117.37, training accuracy: 81.0%, validation accuracy: 84.2%\n",
      "iteration: 634 -> loss: 7596.53, training accuracy: 88.0%, validation accuracy: 79.7%\n",
      "iteration: 636 -> loss: 12508.2, training accuracy: 77.0%, validation accuracy: 83.2%\n",
      "iteration: 638 -> loss: 12389.4, training accuracy: 76.0%, validation accuracy: 79.1%\n",
      "iteration: 640 -> loss: 4014.0, training accuracy: 84.0%, validation accuracy: 84.9%\n",
      "iteration: 642 -> loss: 7170.08, training accuracy: 85.0%, validation accuracy: 83.0%\n",
      "iteration: 644 -> loss: 6590.61, training accuracy: 90.0%, validation accuracy: 84.4%\n",
      "iteration: 646 -> loss: 6248.75, training accuracy: 91.0%, validation accuracy: 80.4%\n",
      "iteration: 648 -> loss: 6506.39, training accuracy: 82.0%, validation accuracy: 83.7%\n",
      "iteration: 650 -> loss: 6228.38, training accuracy: 82.0%, validation accuracy: 84.7%\n",
      "iteration: 652 -> loss: 3504.27, training accuracy: 92.0%, validation accuracy: 84.9%\n",
      "iteration: 654 -> loss: 5074.7, training accuracy: 90.0%, validation accuracy: 84.0%\n",
      "iteration: 656 -> loss: 6525.85, training accuracy: 88.0%, validation accuracy: 82.3%\n",
      "iteration: 658 -> loss: 4128.94, training accuracy: 87.0%, validation accuracy: 84.4%\n",
      "iteration: 660 -> loss: 7650.02, training accuracy: 87.0%, validation accuracy: 82.4%\n",
      "iteration: 662 -> loss: 6836.21, training accuracy: 84.0%, validation accuracy: 83.9%\n",
      "iteration: 664 -> loss: 6547.11, training accuracy: 85.0%, validation accuracy: 84.0%\n",
      "iteration: 666 -> loss: 4476.83, training accuracy: 89.0%, validation accuracy: 84.9%\n",
      "iteration: 668 -> loss: 8669.43, training accuracy: 88.0%, validation accuracy: 84.3%\n",
      "iteration: 670 -> loss: 3715.71, training accuracy: 89.0%, validation accuracy: 84.4%\n",
      "iteration: 672 -> loss: 4169.19, training accuracy: 91.0%, validation accuracy: 86.0%\n",
      "iteration: 674 -> loss: 3225.03, training accuracy: 89.0%, validation accuracy: 85.7%\n",
      "iteration: 676 -> loss: 3861.9, training accuracy: 87.0%, validation accuracy: 84.4%\n",
      "iteration: 678 -> loss: 9459.51, training accuracy: 81.0%, validation accuracy: 85.4%\n",
      "iteration: 680 -> loss: 13520.0, training accuracy: 77.0%, validation accuracy: 81.4%\n",
      "iteration: 682 -> loss: 6224.31, training accuracy: 86.0%, validation accuracy: 84.2%\n",
      "iteration: 684 -> loss: 12013.6, training accuracy: 73.0%, validation accuracy: 80.4%\n",
      "iteration: 686 -> loss: 10325.9, training accuracy: 79.0%, validation accuracy: 80.3%\n",
      "iteration: 688 -> loss: 9598.84, training accuracy: 79.0%, validation accuracy: 82.4%\n",
      "iteration: 690 -> loss: 3784.43, training accuracy: 88.0%, validation accuracy: 82.1%\n",
      "iteration: 692 -> loss: 12304.6, training accuracy: 83.0%, validation accuracy: 81.3%\n",
      "iteration: 694 -> loss: 11688.0, training accuracy: 82.0%, validation accuracy: 83.6%\n",
      "iteration: 696 -> loss: 9203.07, training accuracy: 78.0%, validation accuracy: 78.4%\n",
      "iteration: 698 -> loss: 7918.86, training accuracy: 79.0%, validation accuracy: 83.3%\n",
      "iteration: 700 -> loss: 8909.44, training accuracy: 83.0%, validation accuracy: 80.9%\n",
      "iteration: 702 -> loss: 11084.4, training accuracy: 80.0%, validation accuracy: 85.1%\n",
      "iteration: 704 -> loss: 11208.0, training accuracy: 81.0%, validation accuracy: 76.6%\n",
      "iteration: 706 -> loss: 13676.1, training accuracy: 71.0%, validation accuracy: 79.3%\n",
      "iteration: 708 -> loss: 9560.09, training accuracy: 76.0%, validation accuracy: 76.9%\n",
      "iteration: 710 -> loss: 12563.0, training accuracy: 74.0%, validation accuracy: 79.3%\n",
      "iteration: 712 -> loss: 9710.68, training accuracy: 82.0%, validation accuracy: 80.1%\n",
      "iteration: 714 -> loss: 12823.6, training accuracy: 82.0%, validation accuracy: 73.2%\n",
      "iteration: 716 -> loss: 3320.87, training accuracy: 89.0%, validation accuracy: 85.1%\n",
      "iteration: 718 -> loss: 9527.69, training accuracy: 85.0%, validation accuracy: 78.3%\n",
      "iteration: 720 -> loss: 12776.7, training accuracy: 73.0%, validation accuracy: 85.1%\n",
      "iteration: 722 -> loss: 10061.9, training accuracy: 78.0%, validation accuracy: 75.0%\n",
      "iteration: 724 -> loss: 9699.79, training accuracy: 83.0%, validation accuracy: 79.9%\n",
      "iteration: 726 -> loss: 6301.61, training accuracy: 87.0%, validation accuracy: 81.9%\n",
      "iteration: 728 -> loss: 7615.47, training accuracy: 81.0%, validation accuracy: 81.8%\n",
      "iteration: 730 -> loss: 6663.2, training accuracy: 81.0%, validation accuracy: 85.0%\n",
      "iteration: 732 -> loss: 6645.35, training accuracy: 86.0%, validation accuracy: 77.6%\n",
      "iteration: 734 -> loss: 11461.5, training accuracy: 81.0%, validation accuracy: 84.3%\n",
      "iteration: 736 -> loss: 6883.26, training accuracy: 82.0%, validation accuracy: 79.1%\n",
      "iteration: 738 -> loss: 12460.0, training accuracy: 73.0%, validation accuracy: 81.8%\n",
      "iteration: 740 -> loss: 6888.93, training accuracy: 81.0%, validation accuracy: 77.4%\n",
      "iteration: 742 -> loss: 9374.98, training accuracy: 74.0%, validation accuracy: 78.9%\n",
      "iteration: 744 -> loss: 4276.47, training accuracy: 92.0%, validation accuracy: 83.8%\n",
      "iteration: 746 -> loss: 6399.53, training accuracy: 85.0%, validation accuracy: 83.2%\n",
      "iteration: 748 -> loss: 13386.9, training accuracy: 78.0%, validation accuracy: 84.4%\n",
      "iteration: 750 -> loss: 10315.8, training accuracy: 82.0%, validation accuracy: 85.1%\n",
      "iteration: 752 -> loss: 3299.33, training accuracy: 93.0%, validation accuracy: 86.8%\n",
      "iteration: 754 -> loss: 5854.83, training accuracy: 84.0%, validation accuracy: 85.2%\n",
      "iteration: 756 -> loss: 13003.7, training accuracy: 80.0%, validation accuracy: 83.5%\n",
      "iteration: 758 -> loss: 11123.5, training accuracy: 77.0%, validation accuracy: 84.9%\n",
      "iteration: 760 -> loss: 7016.04, training accuracy: 85.0%, validation accuracy: 84.0%\n",
      "iteration: 762 -> loss: 6708.8, training accuracy: 90.0%, validation accuracy: 84.3%\n",
      "iteration: 764 -> loss: 9446.19, training accuracy: 78.0%, validation accuracy: 83.6%\n",
      "iteration: 766 -> loss: 8562.35, training accuracy: 81.0%, validation accuracy: 85.8%\n",
      "iteration: 768 -> loss: 6740.03, training accuracy: 87.0%, validation accuracy: 81.4%\n",
      "iteration: 770 -> loss: 7164.38, training accuracy: 74.0%, validation accuracy: 83.9%\n",
      "iteration: 772 -> loss: 3850.39, training accuracy: 90.0%, validation accuracy: 85.8%\n",
      "iteration: 774 -> loss: 6495.95, training accuracy: 87.0%, validation accuracy: 83.5%\n",
      "iteration: 776 -> loss: 5779.68, training accuracy: 92.0%, validation accuracy: 85.7%\n",
      "iteration: 778 -> loss: 6493.7, training accuracy: 84.0%, validation accuracy: 84.4%\n",
      "iteration: 780 -> loss: 2848.32, training accuracy: 89.0%, validation accuracy: 85.1%\n",
      "iteration: 782 -> loss: 4488.14, training accuracy: 86.0%, validation accuracy: 86.7%\n",
      "iteration: 784 -> loss: 8125.93, training accuracy: 84.0%, validation accuracy: 85.7%\n",
      "iteration: 786 -> loss: 8069.68, training accuracy: 87.0%, validation accuracy: 84.7%\n",
      "iteration: 788 -> loss: 4333.14, training accuracy: 83.0%, validation accuracy: 85.5%\n",
      "iteration: 790 -> loss: 6440.03, training accuracy: 87.0%, validation accuracy: 87.5%\n",
      "iteration: 792 -> loss: 2288.23, training accuracy: 86.0%, validation accuracy: 86.0%\n",
      "iteration: 794 -> loss: 4665.15, training accuracy: 85.0%, validation accuracy: 87.0%\n",
      "iteration: 796 -> loss: 3734.05, training accuracy: 87.0%, validation accuracy: 84.9%\n",
      "iteration: 798 -> loss: 4101.94, training accuracy: 90.0%, validation accuracy: 87.3%\n",
      "iteration: 800 -> loss: 5305.21, training accuracy: 86.0%, validation accuracy: 85.1%\n",
      "iteration: 802 -> loss: 5012.42, training accuracy: 85.0%, validation accuracy: 86.8%\n",
      "iteration: 804 -> loss: 5287.42, training accuracy: 80.0%, validation accuracy: 84.1%\n",
      "iteration: 806 -> loss: 2965.54, training accuracy: 91.0%, validation accuracy: 83.7%\n",
      "iteration: 808 -> loss: 8493.59, training accuracy: 84.0%, validation accuracy: 83.3%\n",
      "iteration: 810 -> loss: 3705.16, training accuracy: 91.0%, validation accuracy: 86.3%\n",
      "iteration: 812 -> loss: 5391.2, training accuracy: 89.0%, validation accuracy: 86.4%\n",
      "iteration: 814 -> loss: 4359.8, training accuracy: 86.0%, validation accuracy: 82.9%\n",
      "iteration: 816 -> loss: 6983.44, training accuracy: 85.0%, validation accuracy: 87.4%\n",
      "iteration: 818 -> loss: 3038.81, training accuracy: 90.0%, validation accuracy: 83.3%\n",
      "iteration: 820 -> loss: 4538.39, training accuracy: 84.0%, validation accuracy: 83.5%\n",
      "iteration: 822 -> loss: 7061.77, training accuracy: 81.0%, validation accuracy: 88.1%\n",
      "iteration: 824 -> loss: 4582.78, training accuracy: 86.0%, validation accuracy: 87.8%\n",
      "iteration: 826 -> loss: 6629.3, training accuracy: 85.0%, validation accuracy: 87.0%\n",
      "iteration: 828 -> loss: 4494.0, training accuracy: 79.0%, validation accuracy: 87.0%\n",
      "iteration: 830 -> loss: 7612.23, training accuracy: 83.0%, validation accuracy: 81.3%\n",
      "iteration: 832 -> loss: 5964.64, training accuracy: 86.0%, validation accuracy: 85.4%\n",
      "iteration: 834 -> loss: 7150.4, training accuracy: 83.0%, validation accuracy: 80.8%\n",
      "iteration: 836 -> loss: 8661.31, training accuracy: 81.0%, validation accuracy: 85.3%\n",
      "iteration: 838 -> loss: 7085.38, training accuracy: 84.0%, validation accuracy: 85.4%\n",
      "iteration: 840 -> loss: 1932.56, training accuracy: 93.0%, validation accuracy: 84.8%\n",
      "iteration: 842 -> loss: 5272.41, training accuracy: 89.0%, validation accuracy: 87.0%\n",
      "iteration: 844 -> loss: 9265.42, training accuracy: 84.0%, validation accuracy: 83.0%\n",
      "iteration: 846 -> loss: 7124.27, training accuracy: 84.0%, validation accuracy: 84.6%\n",
      "iteration: 848 -> loss: 5033.49, training accuracy: 90.0%, validation accuracy: 84.3%\n",
      "iteration: 850 -> loss: 9976.15, training accuracy: 78.0%, validation accuracy: 86.8%\n",
      "iteration: 852 -> loss: 4521.63, training accuracy: 84.0%, validation accuracy: 84.1%\n",
      "iteration: 854 -> loss: 4476.31, training accuracy: 88.0%, validation accuracy: 86.9%\n",
      "iteration: 856 -> loss: 5286.78, training accuracy: 90.0%, validation accuracy: 87.4%\n",
      "iteration: 858 -> loss: 4921.05, training accuracy: 89.0%, validation accuracy: 85.3%\n",
      "iteration: 860 -> loss: 4709.38, training accuracy: 87.0%, validation accuracy: 84.0%\n",
      "iteration: 862 -> loss: 5668.96, training accuracy: 88.0%, validation accuracy: 87.2%\n",
      "iteration: 864 -> loss: 4636.71, training accuracy: 91.0%, validation accuracy: 86.5%\n",
      "iteration: 866 -> loss: 4642.99, training accuracy: 87.0%, validation accuracy: 86.3%\n",
      "iteration: 868 -> loss: 3570.9, training accuracy: 90.0%, validation accuracy: 87.7%\n",
      "iteration: 870 -> loss: 12107.1, training accuracy: 81.0%, validation accuracy: 87.7%\n",
      "iteration: 872 -> loss: 3179.26, training accuracy: 91.0%, validation accuracy: 87.9%\n",
      "iteration: 874 -> loss: 5033.45, training accuracy: 88.0%, validation accuracy: 86.9%\n",
      "iteration: 876 -> loss: 6597.95, training accuracy: 86.0%, validation accuracy: 88.3%\n",
      "iteration: 878 -> loss: 4236.21, training accuracy: 88.0%, validation accuracy: 85.2%\n",
      "iteration: 880 -> loss: 4524.51, training accuracy: 85.0%, validation accuracy: 88.6%\n",
      "iteration: 882 -> loss: 7332.76, training accuracy: 82.0%, validation accuracy: 86.3%\n",
      "iteration: 884 -> loss: 3429.04, training accuracy: 93.0%, validation accuracy: 86.2%\n",
      "iteration: 886 -> loss: 8513.39, training accuracy: 87.0%, validation accuracy: 85.7%\n",
      "iteration: 888 -> loss: 4394.85, training accuracy: 91.0%, validation accuracy: 88.6%\n",
      "iteration: 890 -> loss: 9133.95, training accuracy: 82.0%, validation accuracy: 86.9%\n",
      "iteration: 892 -> loss: 7156.71, training accuracy: 86.0%, validation accuracy: 88.3%\n",
      "iteration: 894 -> loss: 3317.45, training accuracy: 90.0%, validation accuracy: 86.1%\n",
      "iteration: 896 -> loss: 7424.57, training accuracy: 84.0%, validation accuracy: 87.5%\n",
      "iteration: 898 -> loss: 2224.19, training accuracy: 95.0%, validation accuracy: 89.1%\n",
      "iteration: 900 -> loss: 3590.24, training accuracy: 89.0%, validation accuracy: 86.6%\n",
      "iteration: 902 -> loss: 2234.72, training accuracy: 91.0%, validation accuracy: 86.7%\n",
      "iteration: 904 -> loss: 3057.72, training accuracy: 86.0%, validation accuracy: 87.0%\n",
      "iteration: 906 -> loss: 4574.45, training accuracy: 91.0%, validation accuracy: 83.6%\n",
      "iteration: 908 -> loss: 9177.28, training accuracy: 82.0%, validation accuracy: 86.5%\n",
      "iteration: 910 -> loss: 5900.42, training accuracy: 81.0%, validation accuracy: 88.8%\n",
      "iteration: 912 -> loss: 7031.79, training accuracy: 89.0%, validation accuracy: 86.2%\n",
      "iteration: 914 -> loss: 3897.91, training accuracy: 86.0%, validation accuracy: 87.2%\n",
      "iteration: 916 -> loss: 6806.46, training accuracy: 81.0%, validation accuracy: 85.7%\n",
      "iteration: 918 -> loss: 3799.65, training accuracy: 87.0%, validation accuracy: 90.1%\n",
      "iteration: 920 -> loss: 6108.22, training accuracy: 85.0%, validation accuracy: 87.3%\n",
      "iteration: 922 -> loss: 4018.43, training accuracy: 87.0%, validation accuracy: 85.6%\n",
      "iteration: 924 -> loss: 5581.5, training accuracy: 87.0%, validation accuracy: 85.6%\n",
      "iteration: 926 -> loss: 3577.54, training accuracy: 90.0%, validation accuracy: 85.5%\n",
      "iteration: 928 -> loss: 4337.08, training accuracy: 91.0%, validation accuracy: 87.5%\n",
      "iteration: 930 -> loss: 1810.9, training accuracy: 93.0%, validation accuracy: 88.3%\n",
      "iteration: 932 -> loss: 6234.66, training accuracy: 91.0%, validation accuracy: 87.3%\n",
      "iteration: 934 -> loss: 2443.55, training accuracy: 90.0%, validation accuracy: 87.9%\n",
      "iteration: 936 -> loss: 4700.06, training accuracy: 86.0%, validation accuracy: 88.2%\n",
      "iteration: 938 -> loss: 3053.46, training accuracy: 91.0%, validation accuracy: 85.2%\n",
      "iteration: 940 -> loss: 8209.97, training accuracy: 77.0%, validation accuracy: 89.4%\n",
      "iteration: 942 -> loss: 2827.34, training accuracy: 90.0%, validation accuracy: 87.0%\n",
      "iteration: 944 -> loss: 3797.65, training accuracy: 90.0%, validation accuracy: 87.9%\n",
      "iteration: 946 -> loss: 4788.44, training accuracy: 86.0%, validation accuracy: 86.1%\n",
      "iteration: 948 -> loss: 7267.42, training accuracy: 86.0%, validation accuracy: 88.9%\n",
      "iteration: 950 -> loss: 2754.85, training accuracy: 93.0%, validation accuracy: 85.5%\n",
      "iteration: 952 -> loss: 3044.63, training accuracy: 90.0%, validation accuracy: 89.6%\n",
      "iteration: 954 -> loss: 3413.79, training accuracy: 90.0%, validation accuracy: 89.9%\n",
      "iteration: 956 -> loss: 3600.67, training accuracy: 90.0%, validation accuracy: 89.0%\n",
      "iteration: 958 -> loss: 5301.58, training accuracy: 86.0%, validation accuracy: 90.5%\n",
      "iteration: 960 -> loss: 4431.46, training accuracy: 85.0%, validation accuracy: 89.0%\n",
      "iteration: 962 -> loss: 2129.47, training accuracy: 94.0%, validation accuracy: 90.3%\n",
      "iteration: 964 -> loss: 7166.05, training accuracy: 87.0%, validation accuracy: 88.9%\n",
      "iteration: 966 -> loss: 2166.14, training accuracy: 93.0%, validation accuracy: 88.4%\n",
      "iteration: 968 -> loss: 3680.39, training accuracy: 90.0%, validation accuracy: 88.1%\n",
      "iteration: 970 -> loss: 3120.21, training accuracy: 89.0%, validation accuracy: 91.4%\n",
      "iteration: 972 -> loss: 2200.6, training accuracy: 91.0%, validation accuracy: 92.2%\n",
      "iteration: 974 -> loss: 2187.27, training accuracy: 94.0%, validation accuracy: 87.3%\n",
      "iteration: 976 -> loss: 1930.63, training accuracy: 95.0%, validation accuracy: 89.6%\n",
      "iteration: 978 -> loss: 5283.9, training accuracy: 93.0%, validation accuracy: 90.7%\n",
      "iteration: 980 -> loss: 1425.58, training accuracy: 95.0%, validation accuracy: 89.8%\n",
      "iteration: 982 -> loss: 6118.91, training accuracy: 87.0%, validation accuracy: 89.2%\n",
      "iteration: 984 -> loss: 2604.62, training accuracy: 89.0%, validation accuracy: 91.2%\n",
      "iteration: 986 -> loss: 3345.93, training accuracy: 95.0%, validation accuracy: 90.1%\n",
      "iteration: 988 -> loss: 6275.39, training accuracy: 85.0%, validation accuracy: 89.4%\n",
      "iteration: 990 -> loss: 2696.51, training accuracy: 89.0%, validation accuracy: 90.5%\n",
      "iteration: 992 -> loss: 3170.51, training accuracy: 88.0%, validation accuracy: 89.6%\n",
      "iteration: 994 -> loss: 4290.58, training accuracy: 82.0%, validation accuracy: 88.4%\n",
      "iteration: 996 -> loss: 1488.91, training accuracy: 93.0%, validation accuracy: 89.3%\n",
      "iteration: 998 -> loss: 4187.44, training accuracy: 88.0%, validation accuracy: 88.0%\n",
      "iteration: 1000 -> loss: 4704.3, training accuracy: 88.0%, validation accuracy: 86.7%\n",
      "\n",
      "test accuracy: 87.3%\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "total_iterations = 1000\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  variable_saver = tf.train.Saver()\n",
    "\n",
    "  for iteration in range(total_iterations + 1):\n",
    "    batch_data = random.sample(training_data, batch_size)\n",
    "    batch_input_pixel_data = [pixel_data for _, pixel_data in batch_data]\n",
    "    batch_labels = [\n",
    "      all_labels[filename.split('-')[3]] for filename, _ in batch_data\n",
    "    ]\n",
    "    batch_training_data = {\n",
    "      tf_training_data: batch_input_pixel_data,\n",
    "      tf_training_labels: batch_labels,\n",
    "    }\n",
    "    _, step_loss, training_predictions = session.run(\n",
    "      [training_step, loss, training_estimate], feed_dict=batch_training_data)\n",
    "  \n",
    "    if (iteration % (total_iterations / 500)) == 0:\n",
    "      training_accuracy = calculate_accuracy(training_predictions, batch_labels)\n",
    "      validation_accuracy = calculate_accuracy(validation_estimate.eval(), validation_labels)\n",
    "      accuracies.append((iteration, training_accuracy, validation_accuracy))\n",
    "      if (iteration % (total_iterations / 400)) == 0:\n",
    "        print 'iteration: %s -> loss: %s, training accuracy: %0.1f%%, validation accuracy: %0.1f%%' % (\n",
    "          iteration, step_loss, training_accuracy, validation_accuracy)\n",
    "      if validation_accuracy > 99:\n",
    "        print 'validation accuracy: %01.f%%' % validation_accuracy\n",
    "        break\n",
    "  variable_saver.save(session, '/tmp/detect-shape-model.ckpt', latest_filename='detect-shape-checkpoint-list')\n",
    "  print '\\ntest accuracy: %0.1f%%' % calculate_accuracy(test_estimate.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "plot the accuracy vs iteration number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEKCAYAAAA8QgPpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f17b2ef0950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iterations, training_accuracies, validation_accuracies = zip(*accuracies)\n",
    "plt.plot(iterations, training_accuracies, 'r-', label='training')\n",
    "plt.plot(iterations, validation_accuracies, 'g-', label='validation')\n",
    "axes = plt.gca()\n",
    "_ = axes.set_ylim([0, 110])\n",
    "_ = plt.xlabel('iteration')\n",
    "_ = plt.ylabel('accuracy')\n",
    "_ = plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
