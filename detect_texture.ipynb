{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "We'll use tensorflow to predict the texture in each image: empty, striped or solid.\n",
    "\n",
    "We should already have `.npy` files in `greyscale-data`.  We'll first load data into various structures for later.  This cell mainly splits the data into training, validation and test folds.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input_directory = 'greyscaled-data'\n",
    "\n",
    "# Load all the data into an array.  Each element is a tuple: (filename, numpy data).\n",
    "# The filename structure is \"<number>-<color>-<texture>-<shape>-<rotation>.png\"\n",
    "# We'll sort the data first so the later shuffle is consistent.\n",
    "all_data = [\n",
    "  (f, np.load(os.path.join(input_directory, f))) for f in os.listdir(input_directory)\n",
    "]\n",
    "all_data_sorted = sorted(all_data, key=lambda element: element[0])\n",
    "random.seed(2)\n",
    "random.shuffle(all_data_sorted)\n",
    "\n",
    "# Save 20% of the data for testing (the final, one-shot evaluation of performance).\n",
    "split_index = int(0.2 * len(all_data_sorted))\n",
    "test_data = all_data_sorted[0:split_index]\n",
    "remaining_data = all_data_sorted[split_index:]\n",
    "\n",
    "# Now save 20% of the remaining data for validation.\n",
    "split_index = int(0.2 * len(remaining_data))\n",
    "validation_data = remaining_data[0:split_index]\n",
    "training_data = remaining_data[split_index:]\n",
    "\n",
    "# For convenience, get all the pixel data into separate arrays.\n",
    "training_pixel_data = [pixel_data for _, pixel_data in training_data]\n",
    "validation_pixel_data = np.array([pixel_data for _, pixel_data in validation_data])\n",
    "test_pixel_data = np.array([pixel_data for _, pixel_data in test_data])\n",
    "\n",
    "# Each filename, in its text, has an embedded type of shape.\n",
    "# As in, \"2-red-empty-oval-45.npy\"\n",
    "# We need to convert those classes (the output ground truth) into label arrays.\n",
    "all_labels = {\n",
    "  'empty': [1., 0., 0.],\n",
    "  'striped': [0., 1., 0.],\n",
    "  'solid': [0., 0., 1.],\n",
    "}\n",
    "training_labels = [\n",
    "  all_labels[filename.split('-')[2]] for filename, _ in training_data\n",
    "]\n",
    "validation_labels = [\n",
    "  all_labels[filename.split('-')[2]] for filename, _ in validation_data\n",
    "]\n",
    "test_labels = [\n",
    "  all_labels[filename.split('-')[2]] for filename, _ in test_data\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "setup tensorflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "learning_rate = 0.00005\n",
    "regularization_factor = 1e-4\n",
    "card_width, card_height = 150, 150\n",
    "first_hidden_layer_size, second_hidden_layer_size, third_hidden_layer_size = 512, 512, 512\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # Setup the training steps.\n",
    "  tf_training_data = tf.placeholder(tf.float32, shape=[None, card_width*card_height])\n",
    "  tf_training_labels = tf.placeholder(tf.float32, shape=[None, 3])\n",
    "  \n",
    "  # Create hidden layers of ReLUs.\n",
    "  first_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([card_width*card_height, first_hidden_layer_size]), name='first_hidden_weights')\n",
    "  first_hidden_biases = tf.Variable(\n",
    "    tf.zeros([first_hidden_layer_size]), name='first_hidden_biases')\n",
    "  first_hidden_layer = tf.nn.relu(tf.matmul(tf_training_data, first_hidden_weights) + first_hidden_biases)\n",
    "  second_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([first_hidden_layer_size, second_hidden_layer_size]), name='second_hidden_weights')\n",
    "  second_hidden_biases = tf.Variable(\n",
    "    tf.zeros([second_hidden_layer_size]), name='second_hidden_biases')\n",
    "  second_hidden_layer = tf.nn.relu(tf.matmul(first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  third_hidden_weights = tf.Variable(\n",
    "    tf.truncated_normal([second_hidden_layer_size, third_hidden_layer_size]), name='third_hidden_weights')\n",
    "  third_hidden_biases = tf.Variable(\n",
    "    tf.zeros([third_hidden_layer_size]), name='third_hidden_biases')\n",
    "  third_hidden_layer = tf.nn.relu(tf.matmul(second_hidden_layer, third_hidden_weights) + third_hidden_biases)\n",
    "  \n",
    "  # Build the output layer.\n",
    "  output_weights = tf.Variable(tf.truncated_normal([third_hidden_layer_size, 3]), name='output_weights')\n",
    "  output_biases = tf.Variable(tf.zeros([3]), name='output_biases')\n",
    "  output_logits = tf.matmul(third_hidden_layer, output_weights) + output_biases\n",
    "  training_estimate = tf.nn.softmax(output_logits)\n",
    "\n",
    "  # Calculate loss and setup the optimizer.\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(output_logits, tf_training_labels))\n",
    "  l2_regularization = (tf.nn.l2_loss(output_weights) +\n",
    "                       tf.nn.l2_loss(first_hidden_weights) +\n",
    "                       tf.nn.l2_loss(second_hidden_weights) +\n",
    "                       tf.nn.l2_loss(third_hidden_weights))\n",
    "  loss += regularization_factor * l2_regularization\n",
    "  training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "  # Setup validation.  We have to reshape into a \"dense tensor\"\n",
    "  # by, essentially, combining this array of arrays into a true matrix.\n",
    "  tf_validation_pixel_data = tf.constant(\n",
    "    validation_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  validation_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_validation_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  validation_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(validation_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  validation_third_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(validation_second_hidden_layer, third_hidden_weights) + third_hidden_biases)\n",
    "  validation_logits = tf.matmul(validation_third_hidden_layer, output_weights) + output_biases\n",
    "  validation_estimate = tf.nn.softmax(validation_logits)\n",
    "\n",
    "  # Setup the final test run.\n",
    "  tf_test_pixel_data = tf.constant(\n",
    "    test_pixel_data.reshape((-1, card_width*card_height)).astype(np.float32))\n",
    "  test_first_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(tf_test_pixel_data, first_hidden_weights) + first_hidden_biases)\n",
    "  test_second_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(test_first_hidden_layer, second_hidden_weights) + second_hidden_biases)\n",
    "  test_third_hidden_layer = tf.nn.relu(\n",
    "    tf.matmul(test_second_hidden_layer, third_hidden_weights) + third_hidden_biases)\n",
    "  test_logits = tf.matmul(test_third_hidden_layer, output_weights) + output_biases\n",
    "  test_estimate = tf.nn.softmax(test_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "aside: create a small function to calculate the accuracy of a set of predictions\n",
    "  \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, ground_truth):\n",
    "  \"\"\"Determine what proportion of predictions are accurate based on ground truth.\"\"\"\n",
    "  correctness = np.sum(np.argmax(predictions, 1) == np.argmax(ground_truth, 1))\n",
    "  return 100. * correctness / predictions.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "and run the optimizer in batches\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 -> loss: 235345.0, training accuracy: 31.0%, validation accuracy: 32.4%\n",
      "iteration: 30 -> loss: 2503.9, training accuracy: 74.0%, validation accuracy: 65.3%\n",
      "iteration: 60 -> loss: 2224.53, training accuracy: 77.0%, validation accuracy: 68.7%\n",
      "iteration: 90 -> loss: 2718.89, training accuracy: 65.0%, validation accuracy: 64.5%\n",
      "iteration: 120 -> loss: 807.605, training accuracy: 91.0%, validation accuracy: 73.7%\n",
      "iteration: 150 -> loss: 691.211, training accuracy: 94.0%, validation accuracy: 73.7%\n",
      "iteration: 180 -> loss: 929.777, training accuracy: 86.0%, validation accuracy: 71.8%\n",
      "iteration: 210 -> loss: 1787.45, training accuracy: 72.0%, validation accuracy: 73.7%\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "total_iterations = 300\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  variable_saver = tf.train.Saver()\n",
    "\n",
    "  for iteration in range(total_iterations + 1):\n",
    "    batch_data = random.sample(training_data, batch_size)\n",
    "    batch_input_pixel_data = [pixel_data for _, pixel_data in batch_data]\n",
    "    batch_labels = [\n",
    "      all_labels[filename.split('-')[2]] for filename, _ in batch_data\n",
    "    ]\n",
    "    batch_training_data = {\n",
    "      tf_training_data: batch_input_pixel_data,\n",
    "      tf_training_labels: batch_labels,\n",
    "    }\n",
    "    _, step_loss, training_predictions = session.run(\n",
    "      [training_step, loss, training_estimate], feed_dict=batch_training_data)\n",
    "  \n",
    "    if (iteration % (total_iterations / 20)) == 0:\n",
    "      training_accuracy = calculate_accuracy(training_predictions, batch_labels)\n",
    "      validation_accuracy = calculate_accuracy(validation_estimate.eval(), validation_labels)\n",
    "      accuracies.append((iteration, training_accuracy, validation_accuracy))\n",
    "      if (iteration % (total_iterations / 10)) == 0:\n",
    "        print 'iteration: %s -> loss: %s, training accuracy: %0.1f%%, validation accuracy: %0.1f%%' % (\n",
    "          iteration, step_loss, training_accuracy, validation_accuracy)\n",
    "  variable_saver.save(session, '/tmp/detect-texture-model.ckpt', latest_filename='detect-texture-checkpoint-list')\n",
    "  print '\\ntest accuracy: %0.1f%%' % calculate_accuracy(test_estimate.eval(), test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "plot the accuracy vs iteration number\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "iterations, training_accuracies, validation_accuracies = zip(*accuracies)\n",
    "plt.plot(iterations, training_accuracies, 'r-', label='training')\n",
    "plt.plot(iterations, validation_accuracies, 'g-', label='validation')\n",
    "axes = plt.gca()\n",
    "_ = axes.set_ylim([0, 110])\n",
    "_ = plt.xlabel('iteration')\n",
    "_ = plt.ylabel('accuracy')\n",
    "_ = plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
